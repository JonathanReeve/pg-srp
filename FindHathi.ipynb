{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Finding instances from one corpus in Hathi\n",
    "\n",
    "This shows how to churn through two corpuses to find copies of one in the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import SRP\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This assumes that you've already created the txtlab file described in the notebook \"Hash a corpus of text files into SRP space\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "guten = SRP.Vector_file(\"pg-srp-vecs.bin\", dims = 640, precision = 4, mode = \"r\").to_matrix(unit_length = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "guten['matrix'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def hathi_chunker(max_size=1000, dims = 640):\n",
    "    hathi = SRP.Vector_file(\"/home/bschmidt/vector_models/hathi.bin\")\n",
    "    id_cache = []\n",
    "    row_cache = np.zeros((max_size, dims),\"<f4\")\n",
    "    for id,row in hathi:\n",
    "        row_cache[len(id_cache)] = row[:dims]/np.linalg.norm(row[:dims])\n",
    "        id_cache.append(id)\n",
    "        if len(id_cache) == max_size:\n",
    "            yield (id_cache, row_cache)\n",
    "            id_cache = []\n",
    "            row_cache = np.zeros((max_size,dims),\"<f4\")        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use an array to store the neighbors of each index: store the top ten items to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trans_guten = guten['matrix'].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.argpartition(np.array([1,2,3,4,3,2,1]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hathi_chunks = hathi_chunker()\n",
    "\n",
    "knn = 30\n",
    "neighbors = []\n",
    "for i in range(len(guten[\"names\"])):\n",
    "    neighbors.append([])\n",
    "    for j in range(knn):\n",
    "        neighbors[-1].append( (0, \"nothing\") )\n",
    "\n",
    "n_chunked = 0\n",
    "\n",
    "for ids,rows in hathi_chunks:\n",
    "    n_chunked += 1\n",
    "    pairwise = np.dot(rows, trans_guten)\n",
    "    close = np.where(pairwise > 0.78)\n",
    "    for hathi_i, guten_i in zip(*close):\n",
    "        sim = pairwise[hathi_i, guten_i]\n",
    "        if sim > neighbors[guten_i][-1][0]:\n",
    "            neighbors[guten_i][-1] = (sim,ids[hathi_i])\n",
    "            neighbors[guten_i].sort(reverse=True)\n",
    "        elif sim > .9:\n",
    "            # Catch everything that close\n",
    "            neighbors[guten_i].append((sim,ids[hathi_i]))\n",
    "            neighbors[guten_i].sort(reverse=True)\n",
    "    if n_chunked % 100 == 0:\n",
    "        print \"checked {},000 in hathi\\r\".format(n_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len([n[0] for n in neighbors if n[0][0] > .9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "neighbors[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "guten_title(guten['names'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pgmeta = pd.read_csv(\"pg-meta.csv\")\n",
    "pgmeta.gid = pgmeta.id.astype(\"int64\")\n",
    "pglookup = dict(zip(pgmeta.id.tolist(), pgmeta.title.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "oput = []\n",
    "import pandas as pd\n",
    "for i in range(len(neighbors)):\n",
    "    guten_id = guten['names'][i]\n",
    "    for i, (dist, htid) in enumerate(neighbors[i]):\n",
    "        if i == 0 or dist is not 0:\n",
    "            oput.append({\"gid\": guten_id, \"htid\": htid, \"dist\": dist})\n",
    "pd.DataFrame(oput).to_csv(\"crosswalk.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## What's not matched?\n",
    "\n",
    "24,000 out of 38,000 are being matched. What's being missed? A random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "printed = 0\n",
    "import random\n",
    "random.seed(0)\n",
    "for i in random.sample(range(len(neighbors)), 200):\n",
    "    guten_id = guten['names'][i]\n",
    "    for i, (dist, htid) in enumerate(neighbors[i]):\n",
    "        if i == 0 and dist == 0:\n",
    "            print(\" * \" + pglookup[int(guten_id)])\n",
    "            printed += 1\n",
    "            if printed > 50:\n",
    "                break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What's *not* scanned in Hathi is as interesting as what is. Many of these are journals where the one or two copies in Hathi probably just doesn't have good OCR--or, possibly, is bound in a different way than the Gutenberg editions.\n",
    "\n",
    "Some are crazily small texts that exist elsewhere in PG: \"The Bible, King James version, Book 52: 1 Thessalonians\"\n",
    "\n",
    "Some are specifically PG texts. Things like just the number e to a gazillion decimal points: http://www.gutenberg.org/files/127/127.txt \n",
    "\n",
    "Others are translations into non-English languages, especially of the sci-fi that prevails in PG. 'https://nl.wikipedia.org/wiki/Naar_het_middelpunt_der_aarde'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import ujson as json\n",
    "from IPython.display import HTML\n",
    "\n",
    "guten_cache = {}\n",
    "\n",
    "def guten_title(id, force = False):\n",
    "    return pglookup[int(id)]\n",
    "\n",
    "guten_title(guten['names'][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import ujson as json\n",
    "from IPython.display import HTML\n",
    "\n",
    "#hathi_cache = {}\n",
    "\n",
    "def jsonify(id, force = False):\n",
    "    global hathi_cache\n",
    "    if id in hathi_cache and not force:\n",
    "        return hathi_cache[id]\n",
    "    sons = \"\\n\".join(urllib2.urlopen(\"http://catalog.hathitrust.org/api/volumes/brief/htid/%s.json\" %id.replace(\"+\",\":\").replace(\"=\",\"/\")).readlines())\n",
    "    hathi_cache[id] = json.loads(sons)\n",
    "    return hathi_cache[id]\n",
    "\n",
    "def descend(record):\n",
    "    # Parse a hathi API call response.\n",
    "    a = record['records']\n",
    "    try:\n",
    "        return a[a.keys()[0]]\n",
    "    except IndexError:\n",
    "        print record\n",
    "        raise\n",
    "        \n",
    "def pretty_print(htid,text):\n",
    "    output_string = \"\"#u\"<ul>\"\n",
    "    try:\n",
    "        a = descend(jsonify(htid))\n",
    "        a['url'] = u\"https://babel.hathitrust.org/cgi/pt?id=\" + htid\n",
    "        try:\n",
    "            output_string += u\"<li><a href={}>{} ({})</a><br>{}</li>\".format(\n",
    "                a['url'],a['titles'][0].encode(\"ascii\",\"ignore\"),a['publishDates'][0],text.encode(\"ascii\",\"ignore\"))\n",
    "        except:\n",
    "            print a\n",
    "    except IndexError:\n",
    "        print ('no index',p)\n",
    "        pass\n",
    "    except:\n",
    "        print \"\"\n",
    "        raise\n",
    "    return HTML(output_string + \"\")#)\"</ul>\")\n",
    "\n",
    "class Hathi_Book():\n",
    "    def __init__(self,htid,text=\"\"):\n",
    "        self.htid = htid\n",
    "        self.desc = descend(jsonify(htid))\n",
    "        self.text = text\n",
    "    def _repr_html_(self):\n",
    "        self.desc['url'] = u\"https://babel.hathitrust.org/cgi/pt?id=\" + self.htid\n",
    "        output_string = u\"<li><a href={}>{} ({})</a><br>{}</li>\".format(\n",
    "                self.desc['url'],self.desc['titles'][0].encode(\"ascii\",\"ignore\"),self.desc['publishDates'][0],self.text.decode(\"utf-8\",\"ignore\"))\n",
    "        return output_string\n",
    "    def title(self):\n",
    "        return self.desc['titles'][0]\n",
    "    \n",
    "Hathi_Book(\"inu.30000026383574\",\"Some sample text to go with, ❤\").title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is code to debug the matches that I find. It's involved in the way that research code can be.\n",
    "\n",
    "Essentially, though, it spends most of its time on data cleaning and cutoff. The big challenge is \n",
    "that I don't want it to flag for me as a problem when Hathi has a \"The Works of Charles Dickens, vol 3\" \n",
    "and the textlab has \"Great Expectations.\"\n",
    "\n",
    "So it doesn't bother to compare matches for uninformative Hathi titles.\n",
    "\n",
    "Then it does some string replacement to normalize words or strings like \"and\", \"roman\", and \"œ\":\n",
    "finally, it can compare the titles from Hathi to see if they're the same as those in the textlab. If not,\n",
    "it prints to console suggesting that we check up.\n",
    "\n",
    "In many cases, this reveals problems in the original data: the textlab called a book \"The Vicar of Wrexham\", but it's actuall *The vicar of Wrexhill*. The machine is a decent proofreader!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nearly = []\n",
    "import IPython\n",
    "for i,neighbor in enumerate(neighbors):\n",
    "    for dist, hathi in neighbor:\n",
    "        if dist > .85:\n",
    "            name = guten_title(guten[\"names\"][i])\n",
    "            nearly.append((dist,name,hathi))\n",
    "            #IPython.display.display(Hathi_Book(nearly[-1][2], u\"similarity of {:02f} to {}\".format(nearly[-1][0], nearly[-1][1]).encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nearly.sort(reverse=True)\n",
    "seen = set()\n",
    "last_dist = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import IPython.display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What are the fourteen books with the most perfect matches? Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(14):\n",
    "    IPython.display.display(Hathi_Book(nearly[i][2], u\"similarity of {:02f} to {}\".format(nearly[i][0], nearly[i][1]).encode(\"utf-8\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IPython.display.display(Hathi_Book(nearly[i][2], u\"similarity of {:02f} to {}\".format(nearly[i][0], nearly[i][1]).encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seen = set([])\n",
    "for dist,name,hathi in nearly:\n",
    "    if dist < .9 and last_dist >= .9:\n",
    "        print \"***seen {} at .1 distance, the conservative cutoff.\".format(len(seen))\n",
    "    if dist < .82 and last_dist >= .82:\n",
    "        print \"***seen {} at .18 distance, the hand-picked cutoff for best performance at this task\".format(len(seen))\n",
    "    last_dist = dist\n",
    "    if name in seen:\n",
    "        # The first match for a book is the best.\n",
    "        continue\n",
    "    try:\n",
    "        hathi_title = Hathi_Book(hathi).title()\n",
    "    except: \n",
    "        continue\n",
    "    broken = False\n",
    "    for workmarker in [\n",
    "        u\"sämmtliche\", u\"Novels and tales\",u\"works of\", \"novels of\",\n",
    "        u\"Werke\", u\"Gesammelte\", u\"Romane und Erzählungen\", \"werke\", \"Romane\", u\"Erzählungen\",\n",
    "        u\"Works\", u\"Life and works\", u\"v.\",u\"O︠e︡uvres\", u\"complètes\", u\"complètes\",\"gesammelt\",u\"Sämmtliche\",\n",
    "        u\"OEuvres\", \"The writings of\", \"Tales and novels\", u\"Œuvres\", \"Waverley novels\", u\"Erzählungen\",\n",
    "        u\"Oeuvres\", \"gesammelte Romane\", \"Standard novels\", \"uvres comple\", u\"sämtliche\", u\"sämliche\",\"Samtliche\",\n",
    "    \"Deutsche Literatur\", \"prose tales\", \"Romans\", \"ovels of\", \"'s works\"\n",
    "        \"in philology\", \"Agora\", # These are both 20C journals I can't check to see if they published an old novel.\n",
    "        \"Dichtungen und Schriften\"]:\n",
    "        if workmarker in hathi_title.lower():\n",
    "            broken = True\n",
    "    if broken:\n",
    "        # Don't make me check \"Works v. 4\"\n",
    "        continue\n",
    "    import sys   \n",
    "    seen.add(name)\n",
    "    mcgill_title = name\n",
    "    mt = mcgill_title.decode(\"utf-8\")\n",
    "    try:\n",
    "        ht = hathi_title.decode(\"utf-8\", errors=\"ignore\")\n",
    "    except UnicodeEncodeError:\n",
    "        ht = \"Error\"\n",
    "        hathi_title = \"Error\"\n",
    "    for find, replace in [\n",
    "        (u\"'\",\"\"),\n",
    "        (u\"œ\", \"oe\"),\n",
    "        (\"the\", \"\"),\n",
    "        (\" \",\"\"),\n",
    "        (u\"è\", \"e\"),\n",
    "        (\"-\",\"\"),\n",
    "        (u\"é\",\"e\"),\n",
    "        (\"man\",\"men\"),\n",
    "        (\"dela\", \"\"),\n",
    "        (\"de\", \"\"),\n",
    "        (\",\", \"\"),\n",
    "        (\":\", \"\"),\n",
    "        (\";\", \"\"),\n",
    "        (u\"ß\",\"ss\"),\n",
    "        (\",roman\",\"\"),\n",
    "        (u\" —\", \"\")\n",
    "    ]:\n",
    "        mt = mt.lower().replace(find, replace)\n",
    "        ht = ht.lower().replace(find, replace)\n",
    "    if mt[:15] in ht or ht[:15] in mt:\n",
    "        sys.stdout.write(\".\")\n",
    "        continue\n",
    "    print u\"\\n{} is {:0.4f} from {} ({})\".format(mcgill_title.decode(\"utf-8\"), dist, hathi_title.decode(\"utf-8\"), hathi)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
